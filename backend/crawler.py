import requests  
from bs4 import BeautifulSoup
import re
import urllib3
import time
from concurrent.futures import ThreadPoolExecutor

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

MAIN_URL = "https://dbg.shopreview.co.kr/usr"
CAMPAIGN_URL_TEMPLATE = "https://dbg.shopreview.co.kr/usr/campaign_detail?csq={}"
THREAD_COUNT = 4


# âœ… ë‚ ì§œ ë¬¸ìì—´ì—ì„œ "09ì¼" ê°™ì€ í˜•íƒœ ì¶”ì¶œ
def extract_day(text):
    match = re.search(r"(\d{1,2})ì¼", text)
    if match:
        return f"{int(match.group(1)):02d}ì¼"
    return None


def get_public_campaigns(session):
    public_campaigns = set()
    for attempt in range(3):
        try:
            response = session.get(MAIN_URL, verify=False, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            scripts = soup.find_all("script")
            for script in scripts:
                matches = re.findall(r'data-csq=["\']?(\d+)', script.text)
                public_campaigns.update(map(int, matches))
            if public_campaigns:
                return public_campaigns
        except requests.exceptions.RequestException:
            time.sleep(5)
    return set()


def fetch_campaign_data(campaign_id, session, public_campaigns, selected_days, exclude_keywords):
    url = CAMPAIGN_URL_TEMPLATE.format(campaign_id)
    try:
        response = session.get(url, verify=False, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        # ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë””ë ‰ì…˜ë˜ë©´ ë¬´ì‹œ
        if soup.find("script", string="window.location.href = '/usr/login_form';"):
            return None

        # âœ… ì°¸ì—¬ ê°€ëŠ¥ ì‹œê°„ ì¶”ì¶œ ë° ë‚ ì§œ í•„í„° ì ìš©
        participation_time = soup.find("button", class_="butn butn-success", disabled=True)
        participation_time = participation_time.text.strip() if participation_time else "ì°¸ì—¬ ê°€ëŠ¥ ì‹œê°„ ì—†ìŒ"
        day_str = extract_day(participation_time)

        if not day_str or day_str not in selected_days:
            return None

        # ìƒí’ˆëª… í•„í„°
        product_name = soup.find("h3")
        product_name = product_name.text.strip() if product_name else "ìƒí’ˆëª… ì—†ìŒ"
        if any(keyword in product_name for keyword in exclude_keywords):
            return None

        # ê°€ê²© ì¶”ì¶œ
        price = "ê°€ê²© ì •ë³´ ì—†ìŒ"
        total_price_section = soup.find(string=re.compile("ì´ ê²°ì œê¸ˆì•¡"))
        if total_price_section:
            price_text = total_price_section.find_next("div", style="text-align:right")
            if price_text:
                price_numeric = re.sub(r"[^\d]", "", price_text.text.strip())
                price = price_numeric if price_numeric else "ê°€ê²© ì •ë³´ ì—†ìŒ"

        # ë˜ë°”ê¸° í¬ì¸íŠ¸ ì¶”ì¶œ
        tobagi_points = "0 P"
        tobagi_section = soup.find(string=re.compile("ë˜ë°”ê¸° í¬ì¸íŠ¸"))
        if tobagi_section:
            points_text = tobagi_section.find_next("div", style="text-align:right")
            if points_text:
                tobagi_points = points_text.text.strip()

        # ìƒí’ˆ êµ¬ë¶„ ì¶”ì¶œ
        product_type = "ìƒí’ˆêµ¬ë¶„ ì—†ìŒ"
        delivery_sections = soup.find_all("div", class_="row col-sm4 col-12")
        for section in delivery_sections:
            title = section.find("div", class_="col-6")
            value = section.find("div", style="text-align:right")
            if title and value and "ë°°ì†¡" in title.text:
                product_type = value.text.strip()
                break

        # ì‡¼í•‘ëª° ì •ë³´ ì¶”ì¶œ
        shop_name = "ì‡¼í•‘ëª° ì •ë³´ ì—†ìŒ"
        shop_section = soup.find("div", class_="col-sm-9")
        if shop_section:
            shop_img = shop_section.find("img")
            if shop_img and "alt" in shop_img.attrs:
                shop_name = shop_img["alt"].strip()

        # ë¦¬ë·° í˜•íƒœ
        text_review = "í¬í†  ë¦¬ë·°"
        review_label = soup.find("label", class_="form-check-label", string="í…ìŠ¤íŠ¸ ë¦¬ë·°")
        if review_label:
            text_review = "í…ìŠ¤íŠ¸ ë¦¬ë·°"

        # âœ… ê²°ê³¼ ë¬¸ìì—´ êµ¬ì„±
        result = f"{product_type} & {text_review} & {shop_name} & {price} & {tobagi_points} & {participation_time} & {product_name} & {url}"

        # âœ… ê³µê°œ / ë¹„ê³µê°œ ë¶„ë¥˜
        if campaign_id in public_campaigns:
            return (None, result)
        return (result, None)

    except requests.exceptions.RequestException:
        return (None, None)


def run_crawler(session_cookie, selected_days, exclude_keywords):
    print("âœ… í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹œì‘")
    session = requests.Session()
    session.cookies.set("PHPSESSID", session_cookie)

    public_campaigns = get_public_campaigns(session)
    print("ğŸ“¦ ê³µê°œ ìº í˜ì¸ ê°œìˆ˜:", len(public_campaigns))

    if not public_campaigns:
        print("âŒ ê³µê°œ ìº í˜ì¸ì´ ì—†ìŒ")
        return [], []

    # âœ… í…ŒìŠ¤íŠ¸ìš© ë²”ìœ„
    start_campaign_id = 40000
    end_campaign_id = 40100

    hidden_campaigns = []
    public_campaign_details = []

    with ThreadPoolExecutor(max_workers=THREAD_COUNT) as executor:
        futures = {
            executor.submit(
                fetch_campaign_data, cid, session, public_campaigns, selected_days, exclude_keywords
            ): cid for cid in range(start_campaign_id, end_campaign_id + 1)
        }
        for future in futures:
            result = future.result()
            if result:
                hidden_result, public_result = result
                if hidden_result:
                    hidden_campaigns.append(hidden_result)
                if public_result:
                    public_campaign_details.append(public_result)

    hidden_campaigns = sorted(hidden_campaigns, key=lambda x: x.split(" & ")[5])
    public_campaign_details = sorted(public_campaign_details, key=lambda x: x.split(" & ")[5])

    return hidden_campaigns, public_campaign_details
